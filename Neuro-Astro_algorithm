""""custom neuron-astrocyte network"""
class NeuroAstro(nn.Module):
    def __init__(self, input_sz, neuron_sz, synapse_sz, glia_sz, gama, tau, output_sz, seed):
        super().__init__()
        if seed is not None:
            # use the provided seed value
            self.seed = seed
        else:
            # use a random seed
            self.seed = random.randint(0, 100)
        self.input_size = input_sz
        self.neuron_size = neuron_sz
        self.synapse_size = synapse_sz
        self.glia_size = glia_sz
        self.output_size = output_sz
        
        self.gama = gama
        self.tau = tau
        self.input = nn.Linear(self.input_size, self.neuron_size, bias=True) #input layer
        self.C = nn.Parameter(torch.Tensor(self.synapse_size), requires_grad = False)  #neuron to synapse connection matrix
        self.D = nn.Parameter(torch.Tensor(self.glia_size, self.synapse_size), requires_grad = False)  #glia to synapse connection matrix
        self.F = nn.Parameter(torch.Tensor(self.glia_size, self.glia_size), requires_grad = False) #glia connection matrix
        self.H = nn.Parameter(torch.Tensor(self.synapse_size,self.glia_size), requires_grad = False) #neuron to glia connection matrix
        self.output = nn.Linear(self.neuron_size,self.output_size, bias=True) #output layer

        # Initializing the parameters to some random values
        with torch.no_grad():  # this is to say that initialization will not be considered when computing the gradient later on
            self.C.normal_(std=1 / np.sqrt(self.synapse_size))
            self.D.normal_(std=1 / np.sqrt(self.synapse_size))
            self.F.normal_(std=1 / np.sqrt(self.glia_size))
            self.H.normal_(std=1 / np.sqrt(self.glia_size))

    def forward(self, inputs, init_states):  
        length = inputs.size(-1)
        hidden_list = torch.zeros(length, self.neuron_size*(self.neuron_size+1)+self.glia_size)
        out_list = torch.zeros(length, self.output_size)
        hidden = torch.zeros(1, self.neuron_size*(self.neuron_size+1)+self.glia_size).cuda()
        x_t=  hidden[:, :self.neuron_size]
        w_t = hidden[:, self.neuron_size:self.neuron_size*(self.neuron_size+1)]
        z_t = hidden[:, self.neuron_size*(self.neuron_size+1):self.neuron_size*(self.neuron_size+1)+self.glia_size]
      

        
        for t in range(length):
            x_t1 = (1- self.gama) * x_t + self.gama * (torch.sigmoid(x_t) @ torch.reshape(w_t, (self.neuron_size, self.neuron_size)) + self.input( inputs[:,t]) )
            w_t1 = (1- self.gama) * w_t + self.gama * (torch.reshape((torch.sigmoid(x_t).reshape(-1,1) @ torch.sigmoid(x_t)), (1,-1)) @ torch.diag(self.C) +  torch.tanh(z_t) @ self.D)
            z_t1 = (1- self.gama * self.tau) * z_t + self.gama * self.tau * (torch.tanh(z_t) @ self.F + torch.reshape((torch.sigmoid(x_t).reshape(-1,1) @torch.sigmoid(x_t)), (1,-1)) @ self.H)

            new_hidden = torch.cat( (x_t1, w_t1, z_t1), dim =1)
            out = self.output(x_t1)
            hidden_list[t] = torch.cat( (x_t1, w_t1, z_t1), dim =1)
            out_list[t] = out 
        return out,  new_hidden, hidden_list, out_list


"""Neuro-astrocyte reinforcement learning algorithm"""
class NeuroAstroRL(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = NeuroAstro(input_sz, neuron_sz, synapse_sz, glia_sz, gama, tau, output_sz, seed).cuda()
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)
        self.n_actions = output_sz
        self.state_size = output_sz
        self.model.C.requires_grad = True
        self.reset()
    
    def reset(self):
        self.N = np.zeros((self.n_actions))
        self.action_bank = (torch.ones(self.n_actions, dtype=torch.float) * 10).cuda()


# training
    def train(self, ds):
        rewards = [0]
        regrets = []
        reward_dict = defaultdict(float)
        
        new_hidden = None
        
        self.t = 0
        
        
        loss = 0
        mean_regret = 0
        cumulative_regret = 0
        cumulative_regrets = []
        for epoch in range(1):      # add multiple epochs into training process
            self.model.train()
  
            for i, (rew, orig_probs, context, mask) in enumerate(ds):
                action, reward, regret, new_hidden, m, logits = self.get_action(rew, orig_probs, mask, ds, context=context,
                                                                             ht=new_hidden)
                m_reward = (reward - reward_dict[0])
                self.t += 1


                loss += - m_reward * m.log_prob(action.cuda())   # loss function L_R=-(r_it,t-bar_r_t)log p_it # neural signals
                
                loss.backward()
                self.optimizer.step()

                loss = 0
                self.optimizer.zero_grad()

                        
                if isinstance(new_hidden, tuple):
                        new_hidden = tuple(nh.detach() for nh in new_hidden)
                else:
                        new_hidden = new_hidden.detach()

                self.N[action] += 1
                regrets.append(float(regret))
                rewards.append(rewards[- 1] + float(reward))

                cumulative_regret += regret
                cumulative_regrets.append(cumulative_regret)  #cumulative regret

                n = self.N.sum().item()
                reward_dict[0] = (reward_dict[0] * (n - 1) + reward) / (n)
                mean_regret = (mean_regret * (n - 1) + regret) / (n)

        return rewards, regrets, cumulative_regrets

    
# choose actions


    def get_action(self, rew, orig_probs, mask, ds, **kwargs):
        old_state = kwargs['ht']
        context = kwargs['context']
        
#         prob = torch.Tensor([0.5, 0.9 * (np.sign(np.sin(2 * np.pi / 500* self.t + 2 * np.pi / 3 ))+1.1)/2.1, 0.2 ])   
        env_state = (torch.max(prob)*5 ).unsqueeze(0).unsqueeze(0).cuda() 


        
        logits,  new_hidden, _, _ = self.model(env_state, old_state)
    
        if self.model.training:
                m = Categorical(logits=logits / 2.0)
                action = m.sample()
        else:
                _, action = torch.max(logits, 2)
                

        action = action.detach().cpu()
        reward = rew[0, action].detach().cpu().item() if self.action_bank.detach().cpu()[action] > 0 else 0

        regret = np.max(orig_probs.squeeze().numpy() * np.clip(self.action_bank.detach().cpu().numpy(), 0, 1)) - \
                 (orig_probs.squeeze().numpy() * np.clip(self.action_bank.detach().cpu().numpy(), 0, 1))[action]        
        


        return action, reward, regret, new_hidden, m, logits
    
