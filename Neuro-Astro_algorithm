""""custom neuron-astrocyte network"""
class NeuroAstro(nn.Module):
    def __init__(self, input_sz, neuron_sz, synapse_sz, glia_sz, gama, tau, output_sz, seed):
        super().__init__()
        if seed is not None:
            # use the provided seed value
            self.seed = seed
        else:
            # use a random seed
            self.seed = random.randint(0, 100)
        self.input_size = input_sz
        self.neuron_size = neuron_sz
        self.synapse_size = synapse_sz
        self.glia_size = glia_sz
        self.output_size = output_sz
        
        self.gama = gama
        self.tau = tau
        self.W_in1 = nn.Parameter(torch.Tensor(self.input_size, self.neuron_size), requires_grad = True) #input matrix
        self.C = nn.Parameter(torch.Tensor(self.synapse_size,), requires_grad = True)  #neuron to synapse connection matrix
        self.D = nn.Parameter(torch.Tensor(self.glia_size, self.synapse_size), requires_grad = True)  #glia to synapse connection matrix
        self.F = nn.Parameter(torch.Tensor(self.glia_size, self.glia_size), requires_grad = True) #glia connection matrix
        self.H = nn.Parameter(torch.Tensor(self.synapse_size,self.glia_size), requires_grad = True) #neuron to glia connection matrix
        self.W_in2 = nn.Parameter(torch.Tensor(input2_sz, glia_sz), requires_grad = True)
        self.output = nn.Linear(self.neuron_size,self.output_size, bias=True) #output layer

        # Initializing the parameters to some random values
        with torch.no_grad():  
            self.C.normal_(std=1 / np.sqrt(self.synapse_size))
            self.D.normal_(std=1 / np.sqrt(self.synapse_size))
            self.F.normal_(std=1 / np.sqrt(self.glia_size))
            self.H.normal_(std=1 / np.sqrt(self.glia_size))

    def forward(self, inputs, init_states):  
        length = inputs.size(-1)
        hidden_list = torch.zeros(length, self.neuron_size*(self.neuron_size+1)+self.glia_size)
        out_list = torch.zeros(length, self.output_size)
        hidden = torch.zeros(1, self.neuron_size*(self.neuron_size+1)+self.glia_size).cuda()
        x_t=  hidden[:, :self.neuron_size]
        w_t = hidden[:, self.neuron_size:self.neuron_size*(self.neuron_size+1)]
        z_t = hidden[:, self.neuron_size*(self.neuron_size+1):self.neuron_size*(self.neuron_size+1)+self.glia_size]
        
        for t in range(length):
            x_t = (1- self.gama) * x_t + self.gama * (torch.sigmoid(x_t) @ w_t.reshape(self.neuron_size, self.neuron_size) + inputs[:,t] @ self.W_in1)
            w_t = (1- self.gama) * w_t + self.gama * ((((torch.sigmoid(x_t)).reshape(-1,1) @ torch.sigmoid(x_t)).reshape(1,-1)) @ torch.diag(self.C) +  torch.tanh(z_t) @ self.D)
            z_t = (1- self.gama * self.tau) * z_t + self.gama * self.tau * (torch.tanh(z_t) @ self.F + (((torch.sigmoid(x_t)).reshape(-1,1) @torch.sigmoid(x_t)).reshape(1,-1)) @ self.H + inputs @ self.W_in2)
            new_hidden = torch.cat( (x_t, w_t, z_t), dim =1)
            out = self.output(x_t)
            hidden_list[t] = torch.cat( (x_t, w_t, z_1), dim =1)
            out_list[t] = out 
        return out,  new_hidden, hidden_list, out_list


"""Neuro-astrocyte reinforcement learning algorithm"""
class NeuroAstroRL(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = NeuroAstro(input_sz, neuron_sz, synapse_sz, glia_sz, gama, tau, output_sz, seed).cuda()
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)
        self.n_actions = output_sz
        self.state_size = output_sz
        self.model.C.requires_grad = True
        self.reset()
    
    def reset(self):
        self.N = np.zeros((self.n_actions))
        self.action_bank = (torch.ones(self.n_actions, dtype=torch.float) * 10).cuda()


# training
    def train(self, ds):
        rewards = [0]
        regrets = []
        reward_dict = defaultdict(float)
        new_hidden = None
        self.t = 0
        loss = 0
        mean_regret = 0
        cumulative_regret = 0
        cumulative_regrets = []
        for epoch in range(1):      # add multiple epochs into training process
            self.model.train()
  
            for i, (rew, orig_probs, context, mask) in enumerate(ds):
                action, reward, regret, new_hidden, m, logits = self.get_action(rew, orig_probs, mask, ds, context=context,
                                                                             ht=new_hidden)
                m_reward = (reward - reward_dict[0])
                self.t += 1
                loss += - m_reward * m.log_prob(action.cuda())   # loss function 
                loss.backward()
                self.optimizer.step()

                loss = 0
                self.optimizer.zero_grad()

                        
                if isinstance(new_hidden, tuple):
                        new_hidden = tuple(nh.detach() for nh in new_hidden)
                else:
                        new_hidden = new_hidden.detach()

                self.N[action] += 1
                regrets.append(float(regret))
                rewards.append(rewards[- 1] + float(reward))

                cumulative_regret += regret
                cumulative_regrets.append(cumulative_regret)  #cumulative regret

                n = self.N.sum().item()
                reward_dict[0] = (reward_dict[0] * (n - 1) + reward) / (n)
                mean_regret = (mean_regret * (n - 1) + regret) / (n)

        return rewards, regrets, cumulative_regrets

    
# choose actions


    def get_action(self, rew, orig_probs, mask, ds, **kwargs):
        old_state = kwargs['ht']
        context = kwargs['context']
        dummy_state = (torch.zeros(1)).unsqueeze(0).cuda()  
        env_cue = dummy_state     # this is only for the stationary case, for non-stationary cases, the cue should be adjusted accordingly {-1,0,1}
        logits,  new_hidden, _, _ = self.model(env_cue, env_cue, old_state)
        if self.model.training:
                m = Categorical(logits=logits / 2.0)
                action = m.sample()
        else:
                _, action = torch.max(logits, 2)
            
        action = action.detach().cpu()
        reward = rew[0, action].detach().cpu().item() if self.action_bank.detach().cpu()[action] > 0 else 0

        regret = np.max(orig_probs.squeeze().numpy() * np.clip(self.action_bank.detach().cpu().numpy(), 0, 1)) - \
                 (orig_probs.squeeze().numpy() * np.clip(self.action_bank.detach().cpu().numpy(), 0, 1))[action]        
        
        return action, reward, regret, new_hidden, m, logits
    
